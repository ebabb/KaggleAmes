---
title: "Project"
author: "Rafael Asuncion and Erik Babb"
date: "May 18, 2017"
output: word_document
---
```{r enable parallel processing}
library(doParallel)
registerDoParallel(makeCluster(detectCores()))
```

Step1: Collecting the data

For this data exploration, we shall be examining the Ames Housing Dataset
```{r Load data}
housing <- read.csv("C:/Users/Rafael Asuncion/Documents/WORK/Capstone/train.csv")
#ousing <- read.csv("C:/Users/eb832f/Documents/Data Science/CSUEB/Spring 2017/Capstone/Raf/train.csv")
#train <- read.csv("C:/Users/eb832f/Documents/Data Science/CSUEB/Spring 2017/Capstone/train.csv")
```

Step2: Examining the data

-observe summary of data. value to be predicted is salesPrice.
-drop id as an independent variable
```{r }
housing <- housing[ , !(names(housing) %in% "Id")] #remove the Id column
str(housing)
```

-examine distribution of sales price
```{r }
summary(housing$SalePrice)
hist(housing$SalePrice, xlab = "Sale Prices", main = "Histogram of Sale Price", col = "green")

```

-examine distribution of property ages
```{r }
hist(housing$YearBuilt, xlab = "Year Built", main = "Histogram of Year Built", col = "blue")

```

-examine LotArea and 1st Floor Square Feet
```{r}
library(ggplot2)
ggplot(data=housing, aes(x=LotArea, y=X1stFlrSF)) + geom_point(color="black")
ggplot(data=housing, aes(x=LotArea, y=X1stFlrSF)) + geom_point(aes(color=SalePrice)) +
  xlim(0, 50000) + 
  ylim(0, 4000)
```

- examine correlations of various independent variables & dependent variable SalePrice.
```{r }
cor(housing[c("SalePrice", "LotArea", "OverallQual", "YearBuilt", "YrSold", "OverallCond" , "GrLivArea", "PoolArea")])

```

-scatterplot matrix for variables

```{r }
pairs(housing[c("SalePrice", "LotArea", "OverallQual", "YearBuilt", "YrSold", "OverallCond" , "GrLivArea", "PoolArea")])

```

- another view of scatterplot for variables
```{r }
library(psych)
pairs.panels(housing[c("SalePrice", "LotArea", "OverallQual", "YearBuilt", "YrSold", "OverallCond" , "GrLivArea", "PoolArea")])
```

- examine missing values
```{r }
#View all the NAs
na_count <- sapply(housing, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

library(Amelia)
missmap(housing, main = "Missing values vs observed")
```

- include NA as a factor for categorical variables with NA (not applicable) as an option
```{r }
housing$Alley <- addNA(housing$Alley)
housing$PoolQC <- addNA(housing$PoolQC)
housing$MiscFeature <- addNA(housing$MiscFeature)
housing$Fence <- addNA(housing$Fence)
housing$FireplaceQu <- addNA(housing$FireplaceQu)
housing$GarageCond <- addNA(housing$GarageCond)
housing$GarageQual <- addNA(housing$GarageQual)
housing$GarageFinish <- addNA(housing$GarageFinish)
housing$GarageType <- addNA(housing$GarageType)
housing$BsmtQual <- addNA(housing$BsmtQual)
housing$BsmtCond <- addNA(housing$BsmtCond)
housing$BsmtExposure <- addNA(housing$BsmtExposure)
housing$BsmtFinType1 <- addNA(housing$BsmtFinType1)
housing$BsmtFinType2 <- addNA(housing$BsmtFinType2)
```

- examine missing values again
```{r }
#View all the NAs
na_count <- sapply(housing, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

library(Amelia)
missmap(housing, main = "Missing values vs observed")
```

- Set 1
- drop LotFrontage, GarageYrBlt, MasVnrArea, MasVnrType, Electrical because of missing values wherein NA not as an option.
```{r }
train_DropNAs <- housing[ , !(names(housing) %in% c("LotFrontage","GarageYrBlt","MasVnrArea","MasVnrType", "Electrical"))]
write.csv(train_DropNAs, file = "train_DropNAs.csv",row.names=FALSE)
```

- Set 2
- Convert NAs to mode in LotFrontage, GarageYrBlt, MasVnrArea, MasVnrType, Electrical instead
```{r }
train_NAsToMode <- housing

mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

train_NAsToMode["LotFrontage"][is.na(train_NAsToMode["LotFrontage"])] <- mode(housing$LotFrontage)
train_NAsToMode["GarageYrBlt"][is.na(train_NAsToMode["GarageYrBlt"])] <- mode(housing$GarageYrBlt)
train_NAsToMode["MasVnrArea"][is.na(train_NAsToMode["MasVnrArea"])] <- mode(housing$MasVnrArea)
train_NAsToMode["MasVnrType"][is.na(train_NAsToMode["MasVnrType"])] <- mode(housing$MasVnrType)
train_NAsToMode["Electrical"][is.na(train_NAsToMode["Electrical"])] <- mode(housing$Electrical)
write.csv(train_NAsToMode, file = "train_NAsToMode.csv",row.names=FALSE)
```

- Set 3
- convert features with data type integer to factor
```{r}
#Convert Integer to Factor
train_IntsToFactors <- housing
train_IntsToFactors$MSSubClass <- as.factor(train_IntsToFactors$MSSubClass)
train_IntsToFactors$KitchenAbvGr <- as.factor(train_IntsToFactors$KitchenAbvGr)
train_IntsToFactors$OverallCond <- as.factor(train_IntsToFactors$OverallCond)
train_IntsToFactors$YrSold <- as.factor(train_IntsToFactors$YrSold)
train_IntsToFactors$MoSold <- as.factor(train_IntsToFactors$MoSold)
write.csv(train_IntsToFactors, file = "train_IntsToFactors.csv",row.names=FALSE)
```


- Set 4
- Remove all nominal variables
```{r}

nomvars <- c("MSZoning", "Street", "Alley", "LandContour", "LotConfig",
             "Neighborhood", "Condition1", "Condition2", "HouseStyle",
             "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd",
             "MasVnrType", "Foundation", "Heating", "CentralAir", "Electrical",
             "GarageType", "PavedDrive", "MiscFeature", "SaleType",
             "SaleCondition", "MSSubClass", "LotShape", "Utilities", "LandSlope", "BldgType",
             "OverallQual", "OverallCond", "ExterQual", "ExterCond", "BsmtQual",
             "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",
             "HeatingQC", "KitchenQual", "Functional", "FireplaceQu",
             "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "Fence",
             "MoSold"
             )
train_NoNomVars <- housing[ , !(names(housing) %in% nomvars)]
write.csv(train_NoNomVars, file = "train_NoNomVars.csv",row.names=FALSE)
```


- Set 5
- Explicitly define ordinality for applicable factor features
```{r}
train_OrdDefined <- housing

#covert NAs to more meaningful name
levels(train_OrdDefined$BsmtQual) <- c(levels(train_OrdDefined$BsmtQual), "noBasement")
levels(train_OrdDefined$BsmtCond) <- c(levels(train_OrdDefined$BsmtCond), "noBasement")
levels(train_OrdDefined$BsmtExposure) <- c(levels(train_OrdDefined$BsmtExposure), "noBasement")
levels(train_OrdDefined$BsmtFinType1) <- c(levels(train_OrdDefined$BsmtFinType1), "noBasement")
levels(train_OrdDefined$BsmtFinType2) <- c(levels(train_OrdDefined$BsmtFinType2), "noBasement")
levels(train_OrdDefined$FireplaceQu) <- c(levels(train_OrdDefined$FireplaceQu), "noFireplace")
levels(train_OrdDefined$GarageFinish) <- c(levels(train_OrdDefined$GarageFinish), "noGarage")
levels(train_OrdDefined$GarageQual) <- c(levels(train_OrdDefined$GarageQual), "noGarage")
levels(train_OrdDefined$GarageCond) <- c(levels(train_OrdDefined$GarageCond), "noGarage")
levels(train_OrdDefined$PoolQC) <- c(levels(train_OrdDefined$PoolQC), "noPool")
levels(train_OrdDefined$Fence) <- c(levels(train_OrdDefined$Fence), "noFence")

lvlOrd <- list(
  LotShape = c("Reg", "IR1", "IR2", "IR3"),
  Utilities = c("AllPub", "NoSewr", "NoSeWa", "ELO"),
  LandSlope = c("Gtl", "Mod", "Sev"),
  BldgType = c("1Fam", "2FmCon", "Duplx", "TwnhsE", "TwnhsI"),
  OverallQual = 10:1,
  OverallCond = 10:1,
  ExterQual = c("Ex", "Gd", "TA", "Fa", "Po"),
  ExterCond = c("Ex", "Gd", "TA", "Fa", "Po"),
  BsmtQual = c("Ex", "Gd", "TA", "Fa", "Po", "noBasement"),
  BsmtCond = c("Ex", "Gd", "TA", "Fa", "Po", "noBasement"),
  BsmtExposure = c("Gd", "Av", "Mn", "No", "noBasement"),
  BsmtFinType1 = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "noBasement"),
  BsmtFinType2 = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "noBasement"),
  HeatingQC = c("Ex", "Gd", "TA", "Fa", "Po"),
  KitchenQual = c("Ex", "Gd", "TA", "Fa", "Po"),
  Functional = c("Typ", "Min1", "Min2", "Mod", "Maj1", "Maj2", "Sev", "Sal"),
  FireplaceQu = c("Ex", "Gd", "TA", "Fa", "Po", "noFireplace"),
  GarageFinish = c("Fin", "RFn", "Unf", "noGarage"),
  GarageQual = c("Ex", "Gd", "TA", "Fa", "Po", "noGarage"),
  GarageCond = c("Ex", "Gd", "TA", "Fa", "Po", "noGarage"),
  PoolQC = c("Ex", "Gd", "TA", "Fa", "noPool"),
  Fence = c("GdPrv", "MnPrv", "GdWo", "MnWw", "noFence"),
  MoSold = 1:12
)
for (j in names(lvlOrd)) {
  train_OrdDefined[[j]] <- factor(train_OrdDefined[[j]], levels = lvlOrd[[j]], ordered = TRUE)
}

write.csv(train_OrdDefined, file = "train_OrdDefined.csv",row.names=FALSE)
```


- splitting data into training, validation and test.
```{r }
set.seed(300)
indx = sample(1:nrow(housing), as.integer(0.60*nrow(housing)))
housing_train = housing[indx,]
housing_test = housing[-indx,]

train1 <- train_DropNAs[indx,]
test1 <- train_DropNAs[-indx,]
test1_actual = test1[, "SalePrice"]

train2 <- train_NAsToMode[indx,]
test2 <- train_NAsToMode[-indx,]
test2_actual = test2[, "SalePrice"]

train3 <- train_IntsToFactors[indx,]
test3 <- train_IntsToFactors[-indx,]
test3_actual = test3[, "SalePrice"]

train4 <- train_NoNomVars[indx,]
test4 <- train_NoNomVars[-indx,]
test4_actual = test4[, "SalePrice"]

train5 <- train_OrdDefined[indx,]
test5 <- train_OrdDefined[-indx,]
test5_actual = test5[, "SalePrice"]

housing_test_actual = housing_test[, "SalePrice"]

#What does this do? train and test are already created above
#indx2 = sample(1:nrow(housing_test), as.integer(0.6*nrow(housing_test)))
#housing_validation = housing_test[-indx2,]
#housing_test = housing_test[indx2,]
```


-Create a table to display the the RSME result values
```{r}
mat = matrix(0, nrow = 5, ncol = 5)
dat = as.data.frame(mat)
colnames(dat) <- c("Basic LM", "Adv LM", "Bagging", "rpart", "Caret RF")
rownames(dat) <- c("train_DropNAs", "train_NAsToMode", "train_IntsToFactors", "train_NoNomVars", "train_OrdDefined")

```

Model A: LINEAR Regression Model 
Step3: Training the model
```{r }
model <- lm(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train)
summary(model)
```

Step4: Measuring Performance

- make predictions and evaluate RMSE
```{r }
my_predictions <- predict(model, housing_test)

rmse <- function(error)
{
    sqrt(mean(error^2))
}


error <- housing_test_actual - my_predictions
rmse(error)
 
```

Step5: Improving Performance

- include more significant predictors. As seen in output, this improves RSquare for training data since more predictors are added.
```{r }
model2 <- lm(SalePrice ~ LotArea + OverallQual + GrLivArea  + YearBuilt + TotalBsmtSF + WoodDeckSF + BedroomAbvGr + KitchenAbvGr + GarageArea  + ScreenPorch , data = housing_train)
summary(model2)
```

- make predictions and evaluate RMSE. 
```{r }
my_predictions2  <-predict(model2, housing_test)

error <- housing_test_actual - my_predictions2
rmse(error)
 
```

ModelB: Regression Tree

Step 3: Training the model

```{r }
library(rpart)
model3_decisiontree1 <- rpart(SalePrice ~ ., data = housing_train)
summary(model3_decisiontree1) 
```

- Step 4: Evaluating Model Performance

- use rpart decision tree and make predictions. examine rmse.
```{r }
my_prediction_decision_tree <- predict(model3_decisiontree1, housing_test)
error_decision_tree <- housing_test_actual - my_prediction_decision_tree
rmse(error_decision_tree)
 
```

- Step 5: Improving Model Performance

- implement a Bagged Regeression Tree

```{r }
library(ipred)
set.seed(300)
model4_decision_tree_ipred <- bagging(SalePrice ~ ., data = housing_train, nbagg = 25)
```

- make predictions and examine rmse. 

```{r }
my_prediction_decision_tree_ipred <- predict(model4_decision_tree_ipred, housing_test)
error_decision_tree_ipred <- housing_test_actual - my_prediction_decision_tree_ipred
rmse(error_decision_tree_ipred)
```

ModelC: Random Forest

Step3: Training the model

- implement a random forest using significant continous variable predictors
```{r }
library(randomForest)
set.seed(300)
rf <- randomForest(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train, na.action = na.omit)
rf
```

Step4 : Evaluating model performance

```{r }
predictions_rf <- predict(rf, housing_test)
error_rf <- housing_test_actual - predictions_rf
rmse(error_rf)
```

Step 5: Improving model performance

- auto-tune random forest
```{r }
library(caret)
#ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10) #slower

ctrl <- trainControl(method = "cv", number = 5) #faster

set.seed(300)
m_rf <- train(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train, method = "rf", trControl = ctrl,
              na.action = na.omit)
m_rf
```

- examine rmse of random forest
```{r }
predictions_mrf <- predict(m_rf, housing_test)
error_mrf <- housing_test_actual - predictions_mrf
rmse(error_mrf)
```

Model D: SvM

Step3: Train SVM Vanilladot model

```{r }
library(kernlab)
model5_svm <- ksvm(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train, kernel = "vanilladot")
model5_svm
```

Step4: Evaluating model performance

- make predictions and examine RMSE 
```{r }
predictions_svm <- predict(model5_svm, housing_test)
error_svm <- housing_test_actual - predictions_svm
rmse(error_svm)
```

Step5: Improving mode performance

- train rbfdot svm
```{r }
library(kernlab)
model5_svm_rbf <- ksvm(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train, kernel = "rbfdot")
predictions_svm_rbf <- predict(model5_svm_rbf, housing_test)
error_svm_rbf <- housing_test_actual - predictions_svm_rbf
rmse(error_svm_rbf)
```

After investigating performance of original housing dataset, we have found that bagged regression trees, random forests and SVM
have the best performance and make the most accurate predictions on the test dataset based on RMSE. 

To enhance performance even more, we will make use of the modified datasets preivously created on bagged regression trees, random forests and SVM and examine performance.

Bagged Regression Tree

- original dataset RMSE
```{r }
rmse(error_decision_tree_ipred)
```

- Set1 Bagged Regression Tree RMSE
```{r }
library(ipred)
set.seed(300)
model_bagged_train1 <- bagging(SalePrice ~ ., data = train1, nbagg = 25)
pred_test1_bagged <- predict(model_bagged_train1, test1)
error_test1_bagged <- test1_actual - pred_test1_bagged
rmse(error_test1_bagged)
```

- Set2 Bagged Regression Tree RMSE
```{r }
set.seed(300)
model_bagged_train2 <- bagging(SalePrice ~ ., data = train2, nbagg = 25)
pred_test2_bagged <- predict(model_bagged_train2, test2)
error_test2_bagged <- test2_actual - pred_test2_bagged
rmse(error_test2_bagged)
```

- Set3 Bagged Regression Tree RMSE
```{r }
set.seed(300)
model_bagged_train3 <- bagging(SalePrice ~ ., data = train3, nbagg = 25)
pred_test3_bagged <- predict(model_bagged_train3, test3)
error_test3_bagged <- test3_actual - pred_test3_bagged
rmse(error_test3_bagged)
```

- Set4 Bagged Regression Tree RMSE
```{r }
set.seed(300)
model_bagged_train4 <- bagging(SalePrice ~ ., data = train4, nbagg = 25)
pred_test4_bagged <- predict(model_bagged_train4, test4)
error_test4_bagged <- test4_actual - pred_test4_bagged
rmse(error_test4_bagged)
```

- Set5 Bagged Regression Tree RMSE
```{r }
model_bagged_train5 <- bagging(SalePrice ~ ., data = train5, nbagg = 25)
pred_test5_bagged <- predict(model_bagged_train5, test5)
error_test5_bagged <- test5_actual - pred_test5_bagged
rmse(error_test5_bagged)
```

Random Forest

- original random forest rmse
```{r }
rmse(error_rf)
```

- Set1 Random Forest
```{r }
model_rf_train1 <- randomForest(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train1, na.action = na.omit)
predictions_rf_train1 <- predict(model_rf_train1, test1)
error_rf_train1 <- test1_actual - predictions_rf_train1
rmse(error_rf_train1)
```

- Set2 Random Forest
```{r }
model_rf_train2 <- randomForest(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train2, na.action = na.omit)
predictions_rf_train2 <- predict(model_rf_train2, test2)
error_rf_train2 <- test2_actual - predictions_rf_train2
rmse(error_rf_train2)
```

- Set3 Random Forest
```{r }
model_rf_train3 <- randomForest(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train3, na.action = na.omit)
predictions_rf_train3 <- predict(model_rf_train3, test3)
error_rf_train3 <- test3_actual - predictions_rf_train3
rmse(error_rf_train3)
```

- Set4 Random Forest
```{r }
model_rf_train4 <- randomForest(SalePrice ~ LotArea + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train4, na.action = na.omit)
predictions_rf_train4 <- predict(model_rf_train4, test4)
error_rf_train4 <- test4_actual - predictions_rf_train4
rmse(error_rf_train4)
```

- Set5 Random Forest
```{r }
model_rf_train5 <- randomForest(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train5, na.action = na.omit)
predictions_rf_train5 <- predict(model_rf_train5, test5)
error_rf_train5 <- test5_actual - predictions_rf_train5
rmse(error_rf_train5)
```

SVM

- original SVM RMSE

```{r }
rmse(error_svm)
```

- Set1 SVM
```{r }
model_svm_train1 <- ksvm(SalePrice ~ ., data = train1, kernel = "vanilladot")
pred_svm_train1 <- predict(model_svm_train1, test1)
error_svm_train1 <- test1_actual - pred_svm_train1
rmse(error_svm_train1)
```

- Set2 SVM
```{r }
model_svm_train2 <- ksvm(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train2, kernel = "vanilladot")
pred_svm_train2 <- predict(model_svm_train2, test2)
error_svm_train2 <- test2_actual - pred_svm_train2
rmse(error_svm_train2)
```

- Set3 SVM
```{r }
model_svm_train3 <- ksvm(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train3, kernel = "vanilladot")
pred_svm_train3 <- predict(model_svm_train3, test3)
error_svm_train3 <- test3_actual - pred_svm_train3
rmse(error_svm_train3)
```

- Set4 SVM
```{r }
model_svm_train4 <- ksvm(SalePrice ~ LotArea + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train4, kernel = "vanilladot")
pred_svm_train4 <- predict(model_svm_train4, test4)
error_svm_train4 <- test4_actual - pred_svm_train4
rmse(error_svm_train4)
```

- Set5 SVM
```{r }
model_svm_train5 <- ksvm(SalePrice ~ LotArea + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = train5, kernel = "vanilladot")
pred_svm_train5 <- predict(model_svm_train5, test5)
error_svm_train5 <- test5_actual - pred_svm_train5
rmse(error_svm_train5)
```

CONCLUSION OF FINDINGS

After testing re-engineered datasets on Bagged Regression Trees, Random Forests and SVM we have found that the lowest RMSE upon testing is when SVM is used on Set1.

- Best Model with lowest Test RMSE
```{r }
model_svm_train1 <- ksvm(SalePrice ~ ., data = train1, kernel = "vanilladot")
pred_svm_train1 <- predict(model_svm_train1, test1)
error_svm_train1 <- test1_actual - pred_svm_train1
rmse(error_svm_train1)
```

