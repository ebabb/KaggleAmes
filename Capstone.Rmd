---
title: "House Prices: Advanced Regression Techniques"
author: "Rafael Asuncion and Erik Babb"
date: "May 18, 2017"
output:
  word_document: default
---

Dataset from https://www.kaggle.com/c/house-prices-advanced-regression-techniques

Step1: Collecting the data

For this data exploration, we shall be examining the Ames Housing Dataset
```{r Load data}
#housing <- read.csv("C:/Users/Rafael Asuncion/Documents/WORK/Capstone/train.csv")
housing <- read.csv("C:/Users/eb832f/Documents/Data Science/CSUEB/Spring 2017/Capstone/Raf/train.csv")
#train <- read.csv("C:/Users/eb832f/Documents/Data Science/CSUEB/Spring 2017/Capstone/train.csv")
```

Step2: Examining the data

-observe summary of data. Value to be predicted is SalesPrice.
-drop ID as an independent variable
```{r }
housing <- housing[ , !(names(housing) %in% "Id")] #remove the Id column
str(housing)
```

-examine distribution of sales price
```{r }
summary(housing$SalePrice)
hist(housing$SalePrice, xlab = "Sale Prices", main = "Histogram of Sale Price", col = "green")

```

-examine distribution of property ages
```{r }
hist(housing$YearBuilt, xlab = "Year Built", main = "Histogram of Year Built", col = "blue")

```

-examine LotArea and 1st Floor Square Feet
```{r}
library(ggplot2)
ggplot(data=housing, aes(x=LotArea, y=X1stFlrSF)) + geom_point(color="black")
ggplot(data=housing, aes(x=LotArea, y=X1stFlrSF)) + geom_point(aes(color=SalePrice)) +
  xlim(0, 50000) + 
  ylim(0, 4000)
```

- examine correlations of various independent variables & dependent variable SalePrice.
```{r }
cor(housing[c("SalePrice", "LotArea", "OverallQual", "YearBuilt", "YrSold", "OverallCond" , "GrLivArea", "PoolArea")])

```

-scatterplot matrix for variables

```{r }
pairs(housing[c("SalePrice", "LotArea", "OverallQual", "YearBuilt", "YrSold", "OverallCond" , "GrLivArea", "PoolArea")])

```

- another view of scatterplot for variables
```{r }
library(psych)
pairs.panels(housing[c("SalePrice", "LotArea", "OverallQual", "YearBuilt", "YrSold", "OverallCond" , "GrLivArea", "PoolArea")])
```

- examine missing values
```{r }
#View all the NAs
na_count <- sapply(housing, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

library(Amelia)
missmap(housing, main = "Missing values vs observed")
```

- include NA as a factor for categorical variables with NA (not applicable) as an option
- convert NA to a meaningful level
```{r }
housing$Alley <- addNA(housing$Alley)
housing$PoolQC <- addNA(housing$PoolQC)
housing$MiscFeature <- addNA(housing$MiscFeature)
housing$Fence <- addNA(housing$Fence)
housing$FireplaceQu <- addNA(housing$FireplaceQu)
housing$GarageCond <- addNA(housing$GarageCond)
housing$GarageQual <- addNA(housing$GarageQual)
housing$GarageFinish <- addNA(housing$GarageFinish)
housing$GarageType <- addNA(housing$GarageType)
housing$BsmtQual <- addNA(housing$BsmtQual)
housing$BsmtCond <- addNA(housing$BsmtCond)
housing$BsmtExposure <- addNA(housing$BsmtExposure)
housing$BsmtFinType1 <- addNA(housing$BsmtFinType1)
housing$BsmtFinType2 <- addNA(housing$BsmtFinType2)

levels(housing$Alley)[is.na(levels(housing$Alley))] <- "noAlley"
levels(housing$PoolQC)[is.na(levels(housing$PoolQC))] <- "noPool"
levels(housing$MiscFeature)[is.na(levels(housing$MiscFeature))] <- "none"
levels(housing$Fence)[is.na(levels(housing$Fence))] <- "noFence"
levels(housing$FireplaceQu)[is.na(levels(housing$FireplaceQu))] <- "noFireplace"
levels(housing$GarageCond)[is.na(levels(housing$GarageCond))] <- "noGarage"
levels(housing$GarageQual)[is.na(levels(housing$GarageQual))] <- "noGarage"
levels(housing$GarageFinish)[is.na(levels(housing$GarageFinish))] <- "noGarage"
levels(housing$GarageType)[is.na(levels(housing$GarageType))] <- "noGarage"
levels(housing$BsmtQual)[is.na(levels(housing$BsmtQual))] <- "noBasement"
levels(housing$BsmtCond)[is.na(levels(housing$BsmtCond))] <- "noBasement"
levels(housing$BsmtExposure)[is.na(levels(housing$BsmtExposure))] <- "noBasement"
levels(housing$BsmtFinType1)[is.na(levels(housing$BsmtFinType1))] <- "noBasement"
levels(housing$BsmtFinType2)[is.na(levels(housing$BsmtFinType2))] <- "noBasement"
```

- examine missing values again
```{r }
#View all the NAs
na_count <- sapply(housing, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

library(Amelia)
missmap(housing, main = "Missing values vs observed")
```

- Set 1
- drop LotFrontage, GarageYrBlt, MasVnrArea, MasVnrType, Electrical because of missing values wherein NA not as an option.
```{r }
train_DropNAs <- housing[ , !(names(housing) %in% c("LotFrontage","GarageYrBlt","MasVnrArea","MasVnrType", "Electrical"))]
write.csv(train_DropNAs, file = "train_DropNAs.csv",row.names=FALSE)
```

- Set 2
- Convert NAs to mode in LotFrontage, GarageYrBlt, MasVnrArea, MasVnrType, Electrical instead
```{r }
train_NAsToMode <- housing

mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

train_NAsToMode["LotFrontage"][is.na(train_NAsToMode["LotFrontage"])] <- as.integer(0) #mode from housing would be NA
train_NAsToMode["GarageYrBlt"][is.na(train_NAsToMode["GarageYrBlt"])] <- as.integer(0) #mode from housing would be NA
train_NAsToMode["MasVnrArea"][is.na(train_NAsToMode["MasVnrArea"])] <- mode(housing$MasVnrArea)
train_NAsToMode["MasVnrType"][is.na(train_NAsToMode["MasVnrType"])] <- mode(housing$MasVnrType)
train_NAsToMode["Electrical"][is.na(train_NAsToMode["Electrical"])] <- mode(housing$Electrical)
write.csv(train_NAsToMode, file = "train_NAsToMode.csv",row.names=FALSE)
```

- Set 3
- convert features with data type integer to factor
```{r}
#Convert Integer to Factor
train_IntsToFactors <- housing
train_IntsToFactors$MSSubClass <- as.factor(train_IntsToFactors$MSSubClass)
train_IntsToFactors$KitchenAbvGr <- as.factor(train_IntsToFactors$KitchenAbvGr)
train_IntsToFactors$OverallCond <- as.factor(train_IntsToFactors$OverallCond)
train_IntsToFactors$YrSold <- as.factor(train_IntsToFactors$YrSold)
train_IntsToFactors$MoSold <- as.factor(train_IntsToFactors$MoSold)
write.csv(train_IntsToFactors, file = "train_IntsToFactors.csv",row.names=FALSE)
```


- Set 4
- Remove all nominal variables
```{r}

nomvars <- c("MSZoning", "Street", "Alley", "LandContour", "LotConfig",
             "Neighborhood", "Condition1", "Condition2", "HouseStyle",
             "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd",
             "MasVnrType", "Foundation", "Heating", "CentralAir", "Electrical",
             "GarageType", "PavedDrive", "MiscFeature", "SaleType",
             "SaleCondition", "MSSubClass", "LotShape", "Utilities", "LandSlope", "BldgType",
             "OverallQual", "OverallCond", "ExterQual", "ExterCond", "BsmtQual",
             "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",
             "HeatingQC", "KitchenQual", "Functional", "FireplaceQu",
             "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "Fence",
             "MoSold"
             )
train_NoNomVars <- housing[ , !(names(housing) %in% nomvars)]
write.csv(train_NoNomVars, file = "train_NoNomVars.csv",row.names=FALSE)
```


- Set 5
- Explicitly define ordinality for applicable factor features
```{r}
train_OrdDefined <- housing

lvlOrd <- list(
  LotShape = c("Reg", "IR1", "IR2", "IR3"),
  Utilities = c("AllPub", "NoSewr", "NoSeWa", "ELO"),
  LandSlope = c("Gtl", "Mod", "Sev"),
  BldgType = c("1Fam", "2FmCon", "Duplx", "TwnhsE", "TwnhsI"),
  OverallQual = 10:1,
  OverallCond = 10:1,
  ExterQual = c("Ex", "Gd", "TA", "Fa", "Po"),
  ExterCond = c("Ex", "Gd", "TA", "Fa", "Po"),
  BsmtQual = c("Ex", "Gd", "TA", "Fa", "Po", "noBasement"),
  BsmtCond = c("Ex", "Gd", "TA", "Fa", "Po", "noBasement"),
  BsmtExposure = c("Gd", "Av", "Mn", "No", "noBasement"),
  BsmtFinType1 = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "noBasement"),
  BsmtFinType2 = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "noBasement"),
  HeatingQC = c("Ex", "Gd", "TA", "Fa", "Po"),
  KitchenQual = c("Ex", "Gd", "TA", "Fa", "Po"),
  Functional = c("Typ", "Min1", "Min2", "Mod", "Maj1", "Maj2", "Sev", "Sal"),
  FireplaceQu = c("Ex", "Gd", "TA", "Fa", "Po", "noFireplace"),
  GarageFinish = c("Fin", "RFn", "Unf", "noGarage"),
  GarageQual = c("Ex", "Gd", "TA", "Fa", "Po", "noGarage"),
  GarageCond = c("Ex", "Gd", "TA", "Fa", "Po", "noGarage"),
  PoolQC = c("Ex", "Gd", "TA", "Fa", "noPool"),
  Fence = c("GdPrv", "MnPrv", "GdWo", "MnWw", "noFence"),
  MoSold = 1:12
)
for (j in names(lvlOrd)) {
  train_OrdDefined[[j]] <- factor(train_OrdDefined[[j]], levels = lvlOrd[[j]], ordered = TRUE)
}

write.csv(train_OrdDefined, file = "train_OrdDefined.csv",row.names=FALSE)
```


- splitting data into training, validation and test.
```{r }
set.seed(300)
indx = sample(1:nrow(housing), as.integer(0.60*nrow(housing)))

housing_train = housing[indx,]
housing_test = housing[-indx,]
housing_test_actual = housing_test[, "SalePrice"]

train1 <- train_DropNAs[indx,]
test1 <- train_DropNAs[-indx,]
test1_actual = test1[, "SalePrice"]

train2 <- train_NAsToMode[indx,]
test2 <- train_NAsToMode[-indx,]
test2_actual = test2[, "SalePrice"]

train3 <- train_IntsToFactors[indx,]
test3 <- train_IntsToFactors[-indx,]
test3_actual = test3[, "SalePrice"]

train4 <- train_NoNomVars[indx,]
test4 <- train_NoNomVars[-indx,]
test4_actual = test4[, "SalePrice"]

train5 <- train_OrdDefined[indx,]
test5 <- train_OrdDefined[-indx,]
test5_actual = test5[, "SalePrice"]

```


-Create a table to display the the RSME result values
```{r}
mat = matrix(0, nrow = 5, ncol = 3)
dat = as.data.frame(mat)
colnames(dat) <- c("Bagged Regression Tree", "Random Forest", "SVM")
rownames(dat) <- c("train_DropNAs", "train_NAsToMode", "train_IntsToFactors", "train_NoNomVars", "train_OrdDefined")

```

Model A: LINEAR Regression Model 
Step3: Training the model
```{r }
model <- lm(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train)
summary(model)
```

Step4: Measuring Performance

- make predictions and evaluate RMSE
```{r }
my_predictions <- predict(model, housing_test)

rmse <- function(error)
{
    sqrt(mean(error^2))
}


error <- housing_test_actual - my_predictions
rmse(error)
 
```

Step5: Improving Performance

- include more significant predictors. As seen in output, this improves RSquare for training data since more predictors are added.
```{r }
model2 <- lm(SalePrice ~ LotArea + OverallQual + GrLivArea  + YearBuilt + TotalBsmtSF + WoodDeckSF + BedroomAbvGr + KitchenAbvGr + GarageArea  + ScreenPorch , data = housing_train)
summary(model2)
```

- make predictions and evaluate RMSE. 
```{r }
my_predictions2  <-predict(model2, housing_test)

error <- housing_test_actual - my_predictions2
rmse(error)
 
```

ModelB: Regression Tree

- Step 3: Training the model
- use rpart decision tree
```{r }
library(rpart)
model3_decisiontree1 <- rpart(SalePrice ~ ., data = housing_train)
summary(model3_decisiontree1) 
```

- Step 4: Evaluating Model Performance
- make predictions and examine rmse
```{r }
my_prediction_decision_tree <- predict(model3_decisiontree1, housing_test)
error_decision_tree <- housing_test_actual - my_prediction_decision_tree
rmse(error_decision_tree)
 
```

- Step 5: Improving Model Performance
- implement a Bagged Regeression Tree using ipred package
```{r }
library(ipred)
set.seed(300)
model4_decision_tree_ipred <- bagging(SalePrice ~ ., data = housing_train, nbagg = 25)
```

- make predictions and examine rmse
```{r }
my_prediction_decision_tree_ipred <- predict(model4_decision_tree_ipred, housing_test)
error_decision_tree_ipred <- housing_test_actual - my_prediction_decision_tree_ipred
rmse(error_decision_tree_ipred)
```

ModelC: Random Forest

Step3: Training the model
- implement a random forest using significant continous variable predictors
```{r }
library(randomForest)
set.seed(300)
rf <- randomForest(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train, na.action = na.omit)
rf
```

Step4 : Evaluating model performance
```{r }
predictions_rf <- predict(rf, housing_test)
error_rf <- housing_test_actual - predictions_rf
rmse(error_rf)
```

Step 5: Improving model performance
- auto-tune random forest
```{r }
library(caret)
#ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10) #slower

ctrl <- trainControl(method = "cv", number = 5) #faster

set.seed(300)
m_rf <- train(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train, method = "rf", trControl = ctrl,
              na.action = na.omit)
m_rf
```

- examine rmse of random forest
```{r }
predictions_mrf <- predict(m_rf, housing_test)
error_mrf <- housing_test_actual - predictions_mrf
rmse(error_mrf)
```

Model D: SvM

Step3: Train SVM Vanilladot model

```{r }
library(kernlab)
model5_svm <- ksvm(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train, kernel = "vanilladot")
model5_svm
```

Step4: Evaluating model performance

- make predictions and examine RMSE 
```{r }
predictions_svm <- predict(model5_svm, housing_test)
error_svm <- housing_test_actual - predictions_svm
rmse(error_svm)
```

Step5: Improving mode performance

- train rbfdot svm
```{r }
library(kernlab)
model5_svm_rbf <- ksvm(SalePrice ~ LotArea + OverallQual + GrLivArea + GarageCars + YearBuilt + TotalBsmtSF + WoodDeckSF, data = housing_train, kernel = "rbfdot")
predictions_svm_rbf <- predict(model5_svm_rbf, housing_test)
error_svm_rbf <- housing_test_actual - predictions_svm_rbf
rmse(error_svm_rbf)
```

After investigating performance of original housing dataset, we have found that bagged regression trees, random forests and SVM
have the best performance and make the most accurate predictions on the test dataset based on RMSE. 

To enhance performance even more, we will make use of the modified datasets preivously created on bagged regression trees, random forests and SVM and examine performance.

Bagged Regression Tree

- original dataset RMSE
```{r }
rmse(error_decision_tree_ipred)
```

- Set1 Bagged Regression Tree RMSE
```{r }
library(ipred)
set.seed(300)
model_bagged_train1 <- bagging(SalePrice ~ ., data = train1, nbagg = 25)
pred_test1_bagged <- predict(model_bagged_train1, test1)
error_test1_bagged <- test1_actual - pred_test1_bagged
dat["train_DropNAs", "Bagged Regression Tree"] <- rmse(error_test1_bagged)
```

- Set2 Bagged Regression Tree RMSE
```{r }
model_bagged_train2 <- bagging(SalePrice ~ ., data = train2, nbagg = 25)
pred_test2_bagged <- predict(model_bagged_train2, test2)
error_test2_bagged <- test2_actual - pred_test2_bagged
dat["train_NAsToMode", "Bagged Regression Tree"] <- rmse(error_test2_bagged)
```

- Set3 Bagged Regression Tree RMSE
```{r }
model_bagged_train3 <- bagging(SalePrice ~ ., data = train3, nbagg = 25)
pred_test3_bagged <- predict(model_bagged_train3, test3)
error_test3_bagged <- test3_actual - pred_test3_bagged
dat["train_IntsToFactors", "Bagged Regression Tree"] <- rmse(error_test3_bagged)
```

- Set4 Bagged Regression Tree RMSE
```{r }
model_bagged_train4 <- bagging(SalePrice ~ ., data = train4, nbagg = 25)
pred_test4_bagged <- predict(model_bagged_train4, test4)
error_test4_bagged <- test4_actual - pred_test4_bagged
dat["train_NoNomVars", "Bagged Regression Tree"] <- rmse(error_test4_bagged)
```

- Set5 Bagged Regression Tree RMSE
```{r }
model_bagged_train5 <- bagging(SalePrice ~ ., data = train5, nbagg = 25)
pred_test5_bagged <- predict(model_bagged_train5, test5)
error_test5_bagged <- test5_actual - pred_test5_bagged
dat["train_OrdDefined", "Bagged Regression Tree"] <- rmse(error_test5_bagged)
```

Random Forest

- original Random Forest RMSE
```{r }
rmse(error_rf)
```

- Set1 Random Forest
```{r}
library(randomForest)
set.seed(300)
model_rf_train1 <- randomForest(SalePrice ~ ., data = train1, na.action = na.omit)
predictions_rf_train1 <- predict(model_rf_train1, test1)
error_rf_train1 <- test1_actual - predictions_rf_train1
dat["train_DropNAs", "Random Forest"] <- rmse(error_rf_train1)
```

- Set2 Random Forest
```{r}
model_rf_train2 <- randomForest(SalePrice ~ ., data = train2, na.action = na.omit)
predictions_rf_train2 <- predict(model_rf_train2, test2)
error_rf_train2 <- test2_actual - predictions_rf_train2
dat["train_NAsToMode", "Random Forest"] <- rmse(error_rf_train2)
```

- Set3 Random Forest
```{r}
model_rf_train3 <- randomForest(SalePrice ~ ., data = train3, na.action = na.omit)
predictions_rf_train3 <- predict(model_rf_train3, test3)
error_rf_train3 <- test3_actual[!is.na(predictions_rf_train3)] - predictions_rf_train3[!is.na(predictions_rf_train3)]
dat["train_IntsToFactors", "Random Forest"] <- rmse(error_rf_train3)

```

- Set4 Random Forest
```{r}
model_rf_train4 <- randomForest(SalePrice ~ ., data = train4, na.action = na.omit)
predictions_rf_train4 <- predict(model_rf_train4, test4)
error_rf_train4 <- test4_actual[!is.na(predictions_rf_train4)] - predictions_rf_train4[!is.na(predictions_rf_train4)]
dat["train_NoNomVars", "Random Forest"] <- rmse(error_rf_train4)
```

- Set5 Random Forest
```{r}
model_rf_train5 <- randomForest(SalePrice ~ ., data = train5, na.action = na.omit)
predictions_rf_train5 <- predict(model_rf_train5, test5)
error_rf_train5 <- test5_actual[!is.na(predictions_rf_train5)] - predictions_rf_train5[!is.na(predictions_rf_train5)]
dat["train_OrdDefined", "Random Forest"] <- rmse(error_rf_train5)
```


SVM

- original SVM RMSE
```{r }
rmse(error_svm)
```

- Set1 SVM
```{r }
set.seed(300)
library(kernlab)
model_svm_train1 <- ksvm(SalePrice ~ ., data = train1, kernel = "vanilladot")
pred_svm_train1 <- predict(model_svm_train1, test1)
error_svm_train1 <- test1_actual - pred_svm_train1
dat["train_DropNAs", "SVM"] <- rmse(error_svm_train1)
```

- Set2 SVM
```{r}
model_svm_train2 <- ksvm(SalePrice ~ ., data = train2, kernel = "vanilladot")
pred_svm_train2 <- predict(model_svm_train2, test2)
error_svm_train2 <- test2_actual - pred_svm_train2
dat["train_NAsToMode", "SVM"] <- rmse(error_svm_train2)
```

- Set3 SVM
```{r }
#default na.action=na.omit which leads to rejection of cases with any missing values
model_svm_train3 <- ksvm(SalePrice ~ ., data = train3, kernel = "vanilladot")
pred_svm_train3 <- predict(model_svm_train3, test3)
error_svm_train3 <- test3_actual[complete.cases(test3)] - pred_svm_train3
dat["train_IntsToFactors", "SVM"] <- rmse(error_svm_train3)
```

- Set4 SVM
```{r }
model_svm_train4 <- ksvm(SalePrice ~ ., data = train4, kernel = "vanilladot")
pred_svm_train4 <- predict(model_svm_train4, test4)
error_svm_train4 <- test4_actual[complete.cases(test4)] - pred_svm_train4
dat["train_NoNomVars", "SVM"] <- rmse(error_svm_train4)
```

- Set5 SVM
```{r }
model_svm_train5 <- ksvm(SalePrice ~ ., data = train5, kernel = "vanilladot")
pred_svm_train5 <- predict(model_svm_train5, test5)
error_svm_train5 <- test5_actual[complete.cases(test5)] - pred_svm_train5
dat["train_OrdDefined", "SVM"] <-rmse(error_svm_train5)
```

CONCLUSION OF FINDINGS
```{r}
dat
min(dat)
```

After testing re-engineered datasets on Bagged Regression Trees, Random Forests and SVM we have found that the lowest RMSE upon testing is when SVM is used on Set1.

- Best Model with lowest Test RMSE
```{r }
model_rf_train1 <- randomForest(SalePrice ~ ., data = train1, na.action = na.omit)
predictions_rf_train1 <- predict(model_rf_train1, test1)
error_rf_train1 <- test1_actual - predictions_rf_train1
rmse(error_rf_train1)
```
